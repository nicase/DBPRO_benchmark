{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http import models\n",
    "from qdrant_client.http.models import Distance, VectorParams\n",
    "from qdrant_client.http.models import PointStruct\n",
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "import time\n",
    "import pickle\n",
    "\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "qdrantClient = QdrantClient(host='localhost', port=6333, timeout=10000000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "def read_dataset():\n",
    "    with open('BASEV_WITH_ATTRIBUTES.pkl', 'rb') as f:\n",
    "        baseV = pickle.load(f)\n",
    "    with open('QUERYV_WITH_ATTRIBUTES.pkl', 'rb') as f:\n",
    "        queryV = pickle.load(f)\n",
    "    with open('AF_SIFT_COSINE_GT.pkl', 'rb') as f:\n",
    "        groundTruth = pickle.load(f)\n",
    "    return baseV, queryV, groundTruth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_size = 128\n",
    "collection_name = \"testSIFTT\"\n",
    "\n",
    "qdrantClient.delete_collection(collection_name=collection_name)\n",
    "\n",
    "qdrantClient.recreate_collection(\n",
    "    collection_name=collection_name,\n",
    "    vectors_config=VectorParams(size=vector_size, distance=Distance.COSINE),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading file: siftsmall_base.fvecs\n",
      "    The dimension of the vectors in the file is: 128\n",
      "    The final shape of the loaded dataset siftsmall_base.fvecs is (10000, 128).\n",
      "Loading file: siftsmall_query.fvecs\n",
      "    The dimension of the vectors in the file is: 128\n",
      "    The final shape of the loaded dataset siftsmall_query.fvecs is (100, 128).\n",
      " Loading file: siftsmall_groundtruth.ivecs\n",
      "    The dimension of the vectors in the file is: 100\n",
      "    The final shape of the loaded dataset is (100, 100).\n"
     ]
    }
   ],
   "source": [
    "baseV, queryV, _ = read_dataset()\n",
    "# baseV = baseV[:10000]\n",
    "# queryV = queryV[:100]\n",
    "# baseV = pd.DataFrame({'vector': baseV[:10000]})\n",
    "# queryV = pd.DataFrame({'vector': queryV[:100]})\n",
    "with open('ANN_SIFT_COSINE_GT.pkl', 'rb') as f:\n",
    "        groundTruth = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n"
     ]
    }
   ],
   "source": [
    "baseV, queryV, groundTruth = read_dataset()\n",
    "print(type(baseV.iloc[0][\"vector\"][0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_points = [PointStruct(id=i, vector=elem[\"vector\"], payload= {\"attr1\": elem[\"attr1\"], \"attr2\": elem[\"attr2\"], \"attr3\": elem[\"attr3\"]}) for i, elem in baseV.iterrows()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 50000\n",
    "n = len(batch_points)\n",
    "num_batches = n // batch_size + int(n % batch_size > 0)\n",
    "\n",
    "for batch_idx in range(num_batches):\n",
    "    start_idx = batch_idx * batch_size\n",
    "    end_idx = min((batch_idx + 1) * batch_size, n)\n",
    "\n",
    "    batch_points_i = batch_points[start_idx:end_idx]\n",
    "\n",
    "    operation_info = qdrantClient.upsert(\n",
    "        collection_name=collection_name,\n",
    "        wait=True,\n",
    "        points=batch_points_i\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search function starting\n",
      "Progress: 99/100\r"
     ]
    }
   ],
   "source": [
    "print(f'Search function starting')\n",
    "result_ids = []\n",
    "for i,elem in queryV.iterrows():\n",
    "    print(f'Progress: {i}/{len(queryV)}', end='\\r')\n",
    "    # print(elem)\n",
    "    vec = elem[\"vector\"]\n",
    "    attr1 = elem[\"attr1\"]\n",
    "    attr2 = elem[\"attr2\"]\n",
    "    attr3 = elem[\"attr3\"]\n",
    "    # print(attr1, attr2, attr3)\n",
    "    search_result = qdrantClient.search(\n",
    "        collection_name=collection_name,\n",
    "        query_vector=vec, \n",
    "        query_filter=models.Filter(\n",
    "            # must = AND\n",
    "            must=[\n",
    "                models.FieldCondition(\n",
    "                    key=\"attr1\",\n",
    "                    match=models.MatchValue(\n",
    "                        value=attr1,\n",
    "                    ),\n",
    "                ),\n",
    "                models.FieldCondition(\n",
    "                    key=\"attr2\",\n",
    "                    match=models.MatchValue(\n",
    "                        value=attr2,\n",
    "                    ),\n",
    "                ),\n",
    "                models.FieldCondition(\n",
    "                    key=\"attr3\",\n",
    "                    match=models.MatchValue(\n",
    "                        value=attr3,\n",
    "                    ),\n",
    "                )\n",
    "            ]\n",
    "        ),\n",
    "        limit=100\n",
    "    )\n",
    "    result_ids.append([elem.id for elem in search_result])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "10000\n",
      "Average recall: 1.0\n"
     ]
    }
   ],
   "source": [
    "true_positives = 0\n",
    "n_classified = 0\n",
    "for i,elem in enumerate(result_ids):\n",
    "    true_positives_iter = len(np.intersect1d(groundTruth[i], result_ids[i]))\n",
    "    true_positives += true_positives_iter\n",
    "    n_classified += len(elem)\n",
    "print(true_positives)\n",
    "print(n_classified)\n",
    "print(f'Average recall: {true_positives/n_classified}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
